{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c620acd-18ab-4824-ad84-f8fe3bd3f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy==1.24.4 Pillow==10.3.0 Requests==2.31.0 torch torchvision accelerate jinja2>3.1 qwen-vl-utils av git+https://github.com/huggingface/transformers.git\n",
    "# pip install git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\n",
    "# pip install ultralytics supervision\n",
    "# pip install pybind11\n",
    "# sudo apt-get install build-essential\n",
    "# sudo apt-get install python3-dev\n",
    "# python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc08f4-f94e-49da-a358-06fff3822cf6",
   "metadata": {},
   "source": [
    "# Step 1: Detect a person in region of interest e.g. checkout area\n",
    "Using YOLOv8, we can define a region of interest, so analysis can start when a person is in that zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337d1744-2dac-47bb-9d33-048905355314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "import ultralytics\n",
    "import supervision as sv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54ec26c-6ca7-4f9d-8926-953582414445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO('yolov8s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428e8ae1-45d9-4156-99cb-b719ffa9c940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoInfo(width=1080, height=1920, fps=30, total_frames=302)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Video processing and metedata about the video file\n",
    "ROOT_DIR = \"/home/ubuntu\" \n",
    "vid_path = \"video-3.mp4\"\n",
    "\n",
    "sv.VideoInfo.from_video_path(vid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b2ee69-7574-453b-8cea-78488fe36ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process a video frame by frame, \n",
    "#perform object detection using the YOLO model,&\n",
    "#annotate the frames with detection boxes, labels, and trigger actions based on a polygon zone.\n",
    "\n",
    "# initiate polygon zone\n",
    "polygon = np.array([(900, 0), (1080, 0), (1080, 1920), (100, 1920)])\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(f\"{ROOT_DIR}/{vid_path}\")   # This uses the VideoInfo class from the supervision library (sv) to retrieve metadata from the video file located at the path specified \n",
    "zone = sv.PolygonZone(polygon=polygon) # initializes a Polygon Zone, which is a region of interest (ROI) in the video frame defined by the previously created polygon.\n",
    "\n",
    "# initiate annotators\n",
    "box_annotator = sv.BoxAnnotator(thickness=4)\n",
    "label_annotator = sv.LabelAnnotator(text_thickness=4, text_scale=2)\n",
    "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.WHITE, thickness=6, text_thickness=6, text_scale=4)\n",
    "\n",
    "def process_frame(frame: np.ndarray, _) -> np.ndarray:\n",
    "    # detect\n",
    "    results = model(frame, imgsz=320)[0]\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "    detections = detections[detections.class_id == 0]\n",
    "    zone.trigger(detections=detections)\n",
    "\n",
    "    # annotate\n",
    "    labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, _, confidence, class_id, _, _ in detections]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "    frame = label_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    frame = zone_annotator.annotate(scene=frame)\n",
    "\n",
    "    return frame\n",
    "\n",
    "sv.process_video(source_path=vid_path, target_path=f\"{ROOT_DIR}/out_{vid_path}\", callback=process_frame)\n",
    "\n",
    "from IPython import display\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bfc4b-b047-4fab-aca3-e6d62a6c4d92",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "By defining our **polygon coordinates** we create a zone of interest (the white box in the middle)\n",
    "\n",
    "We see that our count is zero in the left frame, where the customer has not entered the zone\n",
    "\n",
    "And the count increases to 1 as they enter\n",
    "\n",
    "Since we have this information programmtically, we can save the next 20 seconds of video feed to capture all the activities that happened. Only then do we need to analyze the occurrence, saving us on inference cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a4502",
   "metadata": {},
   "source": [
    "Example scene 1\n",
    "<br>\n",
    "<p float=\"left\">\n",
    "  <img src=\"media/scene1-example1.png\" alt=\"Image 1\" width=\"45%\" style=\"margin-right: 5%;\" />\n",
    "  <img src=\"media/scene1-example2.png\" alt=\"Image 2\" width=\"45%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f978e4d",
   "metadata": {},
   "source": [
    "Example scene 2 \n",
    "<br>\n",
    "<img src=\"media/scene2-example.png\" alt=\"Image 2\" width=\"45%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14d521",
   "metadata": {},
   "source": [
    "Example scene 3\n",
    "<br>\n",
    "<p float=\"left\">\n",
    "  <img src=\"media/scene3-example1.png\" alt=\"Image 1\" width=\"45%\" style=\"margin-right: 5%;\" />\n",
    "  <img src=\"media/scene3-example2.png\" alt=\"Image 2\" width=\"45%\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdefa1-8af8-43b6-8c34-2fac7b3cac43",
   "metadata": {},
   "source": [
    "Basically, we would need to map out the coordinates for every camera the client has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aaf779-bac6-440c-b025-61a44aeb0dd7",
   "metadata": {},
   "source": [
    "# Step 2: Ask the VLM to tell us if there is theft in the scene or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db69a24-a348-4bc5-9113-b1cf8eacd0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731539786.004284   11123 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731539786.009218   11123 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e021181-47a6-42dc-b720-0bb6eb935357",
   "metadata": {},
   "source": [
    "## Zero Shot\n",
    "\n",
    "In zero shot, we directly ask the model to tell us if there is any shoplifting in the video or not, without providing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4dce1-338d-4da4-92b9-d1e67d1dcab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qwen-vl-utils using torchvision to read video.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "No<br>reason: The video shows a man shopping in a store and placing items in a basket. There is no indication of shoplifting or any suspicious activity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/home/ubuntu/video-3.mp4\",\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"\n",
    "                    Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\n",
    "                    \"\"\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input for inference, only including video-3\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Processing Vision inputs. Process only the user video (video-6) by passing the relevant part of `messages` directly\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Preparing final inputs\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Model Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "#processing the output\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "# Display the output\n",
    "from IPython.display import Markdown\n",
    "Markdown(output_text[0].replace('\\n', '<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60e41c",
   "metadata": {},
   "source": [
    "## Few Shot\n",
    "\n",
    "To perform few shot prompting, the model is provided with 2 or more examples\n",
    "\n",
    "Here, we mimic the typical user-assistant conversation flow and add the examples as the chat history. This is demonstrated below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1025f37",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<div align=\"right\">\n",
    "\n",
    "**User**  \n",
    "*Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "<br>Reply with two fields\n",
    "<br>answer: Yes or No\n",
    "<br>reason: Your reason\n",
    "<br>Video: 📽️*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Assistant**  \n",
    "*Yes <br> Reason: The man in the video is seen taking items from the cashier's drawer*  \n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"right\">\n",
    "\n",
    "**User**  \n",
    "*Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "<br>Reply with two fields\n",
    "<br>answer: Yes or No\n",
    "<br>reason: Your reason\n",
    "<br>Video: 📽️*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Assistant**  \n",
    "*No <br> Reason: The video shows a man engaging in normal shopping activities with no signs of shoplifting*\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"right\">\n",
    "\n",
    "**User**  \n",
    "*Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "<br>Reply with two fields\n",
    "<br>answer: Yes or No\n",
    "<br>reason: Your reason\n",
    "<br>Video: 📽️*\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "**Assistant**  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e8b2e",
   "metadata": {},
   "source": [
    "With this history of the conversation that includes the **processed video data** and corresponding **manual responses** provided as context, the model continues its generation, leveraging on the examples to align its future outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        # ===================== Example One =====================\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/home/ubuntu/video-6.mp4\",\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    # ===================== Response One (Manually Added) =====================\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"Yes\\nReason: The video shows a man picking office supplies from a table and hiding them in his pockets in a suspicious manner\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    # ===================== Example Two =====================\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/home/ubuntu/video-3.mp4\",\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    # ===================== Response Two (Manually Added) =====================\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"No\\nReason: The video shows one man casually shopping with a basket and another browsing items on the shelf, no strong signs of shoplifting\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    # ===================== Query Video =====================\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/home/ubuntu/video-4.mp4\",\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"\n",
    "                    Is there a strong indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\n",
    "                    \"\"\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input for inference, only including video-3\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# Processing Vision inputs. Process only the user video (video-6) by passing the relevant part of `messages` directly\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# Preparing final inputs\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Model Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "#processing the output\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "# Display the output\n",
    "from IPython.display import Markdown\n",
    "Markdown(output_text[0].replace('\\n', '<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47865a5e",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We setup an evaluation set comprising shoplifting videos from the [UCF Crime Dataset](https://www.crcv.ucf.edu/projects/real-world/) and few other videos\n",
    "\n",
    "We then use the model to analyze each of videos, storing each response for comparison with the ground truth. This is to help us use a scientific approach to comparing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86215750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !gdown \"https://drive.google.com/uc?export=download&id=1_zFhG7g2s4qBU0bCyVHLVyHHAa3jYmqr\"\n",
    "# !unzip shoplifting-videos.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_messages(process_path):\n",
    "    return [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/home/ubuntu/video-6.mp4\",\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"Yes\\nReason: The video shows a man picking office supplies from a table and hiding them in his pockets in a suspicious manner\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"/home/ubuntu/video-3.mp4\",\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"Is there an indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"No\\nReason: The video shows one man casually shopping with a basket and another browsing items on the shelf, no strong signs of shoplifting\"\"\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": process_path,\n",
    "                \"max_pixels\": 360 * 420,\n",
    "                \"fps\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"\"\"\n",
    "                    Is there a strong indication of suspicious store activity in this video such as shoplifting or not. \n",
    "                    Reply with two fields\n",
    "                    answer: Yes or No\n",
    "                    reason: Your reason\n",
    "                    \"\"\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "    \n",
    "\n",
    "def get_video_duration(video_path):\n",
    "    \"\"\"Get the duration of a video in seconds.\"\"\"\n",
    "    try:\n",
    "        clip = VideoFileClip(video_path)\n",
    "        duration = clip.duration\n",
    "        clip.close()\n",
    "        return duration\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error getting video duration: {str(e)}\")\n",
    "\n",
    "def trim_video(input_path, output_path, duration=30):\n",
    "    \"\"\"Trim video to specified duration in seconds.\"\"\"\n",
    "    try:\n",
    "        with VideoFileClip(input_path) as clip:\n",
    "            trimmed_clip = clip.subclip(0, duration)\n",
    "            trimmed_clip.write_videofile(output_path, \n",
    "                                       codec='libx264', \n",
    "                                       audio=False,\n",
    "                                       logger=None)  \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error trimming video: {str(e)}\")\n",
    "\n",
    "def sample_frames_dynamic_v1(video_path, output_path, target_frames=30):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = 1 #cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        duration = total_frames / fps\n",
    "        print(f\"Video duration: {duration:.2f} seconds, Total frames: {total_frames}\")\n",
    "\n",
    "        sampling_interval = max(1, int(total_frames / target_frames))\n",
    "        print(f\"Sampling every {sampling_interval} frames to extract {target_frames} frames.\")\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        frame_count = 0\n",
    "        saved_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count % sampling_interval == 0 and saved_count < target_frames:\n",
    "                out.write(frame)\n",
    "                saved_count += 1\n",
    "            frame_count += 1\n",
    "            if saved_count >= target_frames:\n",
    "                break\n",
    "        cap.release()\n",
    "        out.release()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error sampling and recreating video: {str(e)}\")\n",
    "\n",
    "def sample_frames_dynamic_v2(video_path, output_path, target_frames=30, target_duration=30, output_fps=30):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        input_duration = total_frames / input_fps      \n",
    "        target_frames = target_duration * output_fps\n",
    "        sampling_interval = max(1, total_frames / target_frames)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, output_fps, (width, height))\n",
    "        frame_count = 0\n",
    "        saved_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_count >= saved_count * sampling_interval and saved_count < target_frames:\n",
    "                out.write(frame)\n",
    "                saved_count += 1\n",
    "            frame_count += 1\n",
    "            if saved_count >= target_frames:\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error sampling and recreating video: {str(e)}\")\n",
    "\n",
    "def process_video_directory(video_dir, output_csv, max_duration=30):\n",
    "    results = []\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "    \n",
    "    for video_file in tqdm(video_files):\n",
    "        try:\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            print(f\"\\nProcessing {video_file}...\")\n",
    "            \n",
    "            duration = get_video_duration(video_path)\n",
    "            print(f\"Video duration: {duration:.2f} seconds\")\n",
    "            \n",
    "            if duration > max_duration:\n",
    "                sampled_frames_dir = os.path.join(temp_dir, f\"resampled_{(video_file)}\")\n",
    "                sample_frames_dynamic_v1(video_path, sampled_frames_dir)\n",
    "                process_path = sampled_frames_dir \n",
    "                print(\"Frame sampling complete\")\n",
    "            else:\n",
    "                process_path = video_path\n",
    "\n",
    "            ## uncomment for 30-second trimming\n",
    "\n",
    "            # if duration > max_duration:\n",
    "            #     print(f\"Video longer than {max_duration} seconds, trimming...\")\n",
    "            #     temp_video_path = os.path.join(temp_dir, f\"trimmed_{video_file}\")\n",
    "            #     trim_video(video_path, temp_video_path, max_duration)\n",
    "            #     process_path = temp_video_path\n",
    "            # else:\n",
    "            #     process_path = video_path\n",
    "            \n",
    "            messages = prepare_messages(process_path)\n",
    "\n",
    "            text = processor.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "\n",
    "            # Generate response\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "\n",
    "            response_lines = output_text.strip().split('\\n')\n",
    "            \n",
    "            answer = response_lines[0].split(': ')[-1].strip()\n",
    "            reason = response_lines[1].split(': ')[1].strip()\n",
    "\n",
    "            results.append({\n",
    "                'filename': video_file,\n",
    "                'duration': duration,\n",
    "                'trimmed': duration > max_duration,\n",
    "                'anomaly': answer,\n",
    "                'reason': reason,\n",
    "            })\n",
    "            \n",
    "            print(f\"Analysis complete - Anomaly detected: {answer}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {video_file}: {str(e)}\")\n",
    "            results.append({\n",
    "                'filename': video_file,\n",
    "                'duration': -1,\n",
    "                'trimmed': False,\n",
    "                'anomaly': 'ERROR',\n",
    "                'reason': str(e)\n",
    "            })\n",
    "\n",
    "    try:\n",
    "        import shutil\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"\\nCleaned up temporary directory: {temp_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning up temporary directory: {str(e)}\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nResults saved to {output_csv}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "VIDEO_DIR = \"/home/ubuntu/content/drive/MyDrive/shoplifting-videos\"  \n",
    "OUTPUT_CSV = \"qwen_results.csv\"\n",
    "MAX_DURATION = 30 \n",
    "\n",
    "results_df = process_video_directory(VIDEO_DIR, OUTPUT_CSV, MAX_DURATION)\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "print(f\"Total videos processed: {len(results_df)}\")\n",
    "print(f\"Videos trimmed: {len(results_df[results_df['trimmed']])}\")\n",
    "print(f\"Anomalies detected: {len(results_df[results_df['anomaly'] == 'Yes'])}\")\n",
    "print(f\"Processing errors: {len(results_df[results_df['anomaly'] == 'ERROR'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --csv result.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f8cc1",
   "metadata": {},
   "source": [
    "This calculates the prediction accuracy, precision and recall etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
